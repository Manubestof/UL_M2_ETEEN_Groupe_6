#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PIPELINE UTILITIES - FONCTIONS IND√âPENDANTES
============================================

Ce module contient les fonctions utilitaires n√©cessaires au pipeline,
extraites et adapt√©es de generate_data.py pour assurer l'ind√©pendance totale.
"""

from loguru import logger
import sys
import os
import pandas as pd
import numpy as np
import io
import contextlib
import time
import glob
import comtradeapicall
import pickle
from pathlib import Path
from dotenv import load_dotenv


def get_exports_dataframe(
    input_path: str = "data/exports/",
    year_start: int = 1979,
    year_end: int = 2024,
    max_records: int = 5000,
    replace: bool = False,
    fetch_missing: bool = False,
) -> pd.DataFrame:
    """
    G√©n√®re un DataFrame d'exports filtr√© pour la p√©riode sp√©cifi√©e.

    Version adapt√©e et ind√©pendante extraite de generate_data.py

    Args:
        input_path: Chemin vers les fichiers d'exports
        year_start: Ann√©e de d√©but
        year_end: Ann√©e de fin
        max_records: Nombre max d'enregistrements par requ√™te
        replace: Remplacer les fichiers existants
        fetch_missing: T√©l√©charger les ann√©es manquantes

    Returns:
        DataFrame avec les donn√©es d'exports filtr√©es
    """

    def extract_years_from_filename(filename):
        """Extrait les ann√©es d'un nom de fichier."""
        # Pattern 1: Plage d'ann√©es - YYYY-YYYY_exports (avec suffixes possibles)
        range_match = re.search(r"(\d{4})-(\d{4})_exports", filename)
        if range_match:
            start_year = int(range_match.group(1))
            end_year = int(range_match.group(2))
            return list(range(start_year, end_year + 1))

        # Pattern 2: Ann√©e simple - YYYY_exports (avec suffixes possibles)
        single_year_match = re.search(r"(\d{4})_exports", filename)
        if single_year_match:
            return [int(single_year_match.group(1))]

        return []

    # Rechercher tous les fichiers d'exports
    all_export_files = sorted(glob.glob(f"{input_path}*exports*.csv"))

    # Mapping direct fichier -> ann√©es
    file_coverage = {}
    for file in all_export_files:
        filename = os.path.basename(file)
        years = extract_years_from_filename(filename)
        if years:
            file_coverage[file] = years
            year_range = (
                f"{min(years)}-{max(years)}" if len(years) > 1 else str(years[0])
            )
            logger.trace(f"üìÅ {filename} ‚Üí {year_range}")

    # S√©lectionner les fichiers qui couvrent la p√©riode demand√©e
    target_years = set(range(year_start, year_end + 1))
    files_to_load = []
    years_covered = set()

    for file_path, file_years in file_coverage.items():
        overlap = set(file_years).intersection(target_years)
        if overlap:
            files_to_load.append(file_path)
            years_covered.update(overlap)

            year_range = (
                f"{min(overlap)}-{max(overlap)}"
                if len(overlap) > 1
                else str(list(overlap)[0])
            )
            logger.trace(f"‚úÖ {os.path.basename(file_path)} ‚Üí couvre {year_range}")

    missing_years = sorted(target_years - years_covered)

    # Logs de diagnostic
    logger.debug(
        f"üìä P√©riode demand√©e: {year_start}-{year_end} ({len(target_years)} ann√©es)"
    )
    logger.debug(f"üìÅ Fichiers s√©lectionn√©s: {len(files_to_load)}")

    if years_covered:
        actual_range = (
            f"{min(years_covered)}-{max(years_covered)}"
            if len(years_covered) > 1
            else str(list(years_covered)[0])
        )
        logger.debug(
            f"‚úÖ Ann√©es r√©ellement couvertes: {actual_range} ({len(years_covered)} ann√©es)"
        )
        logger.info(f"Ann√©es d√©j√† pr√©sentes localement: {sorted([int(y) for y in years_covered])}")

    if missing_years:
        logger.warning(f"‚ùå Ann√©es manquantes: {missing_years}")

    # Chargement des fichiers
    if not files_to_load:
        if not fetch_missing:
            logger.warning(
                "‚ùå Aucun fichier trouv√©. Utilisez fetch_missing=True pour t√©l√©charger."
            )
            return pd.DataFrame()
        else:
            logger.warning("üîÑ T√©l√©chargement non support√© dans cette version pipeline")
            return pd.DataFrame()

    # Charger les fichiers existants
    exports_all = []
    for file in files_to_load:
        try:
            df = pd.read_csv(
                file, encoding="latin1", sep=None, engine="python", index_col=False
            )
            exports_all.append(df)
            logger.trace(f"‚úÖ Charg√©: {os.path.basename(file)}")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de {file}: {e}")

    if not exports_all:
        logger.warning("‚ùå Aucun fichier n'a pu √™tre charg√©")
        return pd.DataFrame()

    exports = pd.concat(exports_all, ignore_index=True)
    logger.debug(f"üìä Chargement r√©ussi : {len(exports)} lignes")

    # Nettoyage et pr√©paration
    if exports.empty:
        logger.error("‚ùå Aucune donn√©e d'export disponible")
        return pd.DataFrame()

    # Cr√©er l'indicateur agricole
    exports["is_agri"] = exports["cmdCode"].isin(range(1, 25))

    # Filtrer pour la p√©riode demand√©e et nettoyer
    exports_filtered = exports[
        (exports["refYear"] >= year_start) & (exports["refYear"] <= year_end)
    ][
        [
            "reporterISO",
            "reporterDesc",
            "refYear",
            "cmdCode",
            "cmdDesc",
            "is_agri",
            "fobvalue",
        ]
    ].copy()

    exports_filtered = exports_filtered.dropna(subset=["fobvalue"]).query(
        "fobvalue > 0"
    )

    # R√©sum√© final avec vraies donn√©es
    if not exports_filtered.empty:
        actual_years = sorted(exports_filtered["refYear"].unique())
        actual_range = (
            f"{min(actual_years)}-{max(actual_years)}"
            if len(actual_years) > 1
            else str(actual_years[0])
        )
        n_agri = exports_filtered["is_agri"].sum()
        pct_agri = exports_filtered["is_agri"].mean() * 100

        logger.info(f"üìä EXPORTS DATA ({actual_range}):")
        logger.info(
            f"   {len(exports_filtered):,} obs, {exports_filtered['reporterISO'].nunique()} countries, {exports_filtered['cmdCode'].nunique()} products"
        )
        logger.info(f"   Agricultural: {n_agri:,} ({pct_agri:.1f}%)")
    else:
        logger.warning(f"‚ùå Aucune donn√©e disponible pour {year_start}-{year_end}")

    return exports_filtered

def _check_year_has_data(
    year: int, breakdown_mode: str = "plus", max_records: int = 100
) -> bool:
    """V√©rifie rapidement si une ann√©e a des donn√©es disponibles."""
    try:
        f = io.StringIO()
        with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):
            test_df = comtradeapicall.previewFinalData(
                typeCode="C",
                freqCode="A",
                clCode="HS",
                period=year,
                reporterCode=None,
                cmdCode=None,
                flowCode="X",
                partnerCode="0",
                partner2Code=None,
                customsCode=None,
                motCode=None,
                maxRecords=max_records,
                format_output="JSON",
                aggregateBy=None,
                breakdownMode=breakdown_mode,
                countOnly=None,
                includeDesc=True,
            )

        api_output = f.getvalue()

        if "403" in api_output or "quota" in api_output.lower():
            logger.error(
                f"üö´ QUOTA API √âPUIS√â d√©tect√© lors de la v√©rification de l'ann√©e {year}"
            )
            raise Exception("API_QUOTA_EXCEEDED")

        if test_df is not None and len(test_df) > 0:
            logger.debug(f"‚úÖ Year {year}: Data available ({len(test_df)} records)")
            return True
        else:
            logger.warning(f"‚ùå Year {year}: No data available")
            return False

    except Exception as e:
        if "API_QUOTA_EXCEEDED" in str(e):
            raise e
        logger.warning(f"‚ö†Ô∏è Year {year}: Error during data check - {e}")
        return False

def fetch_comtrade_exports(
    output_path: str,
    breakdown_mode: str = "plus",
    replace: bool = False,
    max_records: int = 5000,
    year_start: int = 1979,
    year_end: int = 2024,
):
    """T√©l√©charge les donn√©es Comtrade pour une p√©riode donn√©e."""
    import re
    start_time = time.time()
    all_dfs_exports = []
    skipped_years = []
    quota_exceeded = False

    os.makedirs(output_path, exist_ok=True)

    # --- NOUVEAU : d√©tection des ann√©es d√©j√† couvertes localement (multi-ann√©es inclus) ---
    export_files = sorted(glob.glob(os.path.join(output_path, '*exports*.csv')))
    years_covered = set()
    def extract_years_from_filename(filename):
        range_match = re.search(r"(\d{4})-(\d{4})_exports", filename)
        if range_match:
            start_year = int(range_match.group(1))
            end_year = int(range_match.group(2))
            return list(range(start_year, end_year + 1))
        single_year_match = re.search(r"(\d{4})_exports", filename)
        if single_year_match:
            return [int(single_year_match.group(1))]
        return []
    for file in export_files:
        years_covered.update(extract_years_from_filename(os.path.basename(file)))
    target_years = set(range(year_start, year_end + 1))
    missing_years = sorted(target_years - years_covered)
    logger.debug(f"Ann√©es d√©j√† couvertes: {sorted(years_covered)}")
    if missing_years:
        logger.info(f"üìÖ Ann√©es √† t√©l√©charger: {missing_years}")
    else:
        logger.debug("‚úÖ Aucune donn√©e manquante")
        return pd.DataFrame()  # Rien √† faire

    # --- Boucle de t√©l√©chargement uniquement sur les ann√©es manquantes ---
    for year in missing_years:
        if quota_exceeded:
            logger.error(
                f"‚ùå Arr√™t √† cause du quota √©puis√©. Donn√©es partielles jusqu'√† l'ann√©e {year-1}"
            )
            break
        output_file = os.path.join(output_path, f"{year}_exports_{breakdown_mode}.csv")
        if not replace and os.path.exists(output_file):
            try:
                existing_df = pd.read_csv(output_file)
                if len(existing_df) > 0:
                    all_dfs_exports.append(existing_df)
                    logger.trace(
                        f"üìÅ Loaded existing data for {year}: {len(existing_df)} records"
                    )
                else:
                    skipped_years.append(year)
            except:
                logger.warning(f"‚ö†Ô∏è Could not load existing file for {year}")
                skipped_years.append(year)
            continue
        # V√©rification avec d√©tection quota
        try:
            has_data = _check_year_has_data(year, breakdown_mode, max_records=100)
            if not has_data:
                skipped_years.append(year)
                continue
        except Exception as e:
            if "API_QUOTA_EXCEEDED" in str(e):
                quota_exceeded = True
                break
            else:
                skipped_years.append(year)
                continue
        # T√©l√©chargement des donn√©es
        logger.info(f"üîÑ Traitement ann√©e {year}...")
        df_year_cmd = []
        for cmd_code in range(1, 100):
            if quota_exceeded:
                break
            try:
                f = io.StringIO()
                with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):
                    df = comtradeapicall.previewFinalData(
                        typeCode="C",
                        freqCode="A",
                        clCode="HS",
                        period=year,
                        reporterCode=None,
                        cmdCode=f"{cmd_code:02}",
                        flowCode="X",
                        partnerCode="0",
                        partner2Code=None,
                        customsCode=None,
                        motCode=None,
                        maxRecords=max_records,
                        format_output="JSON",
                        aggregateBy=None,
                        breakdownMode=breakdown_mode,
                        countOnly=None,
                        includeDesc=True,
                    )
                api_output = f.getvalue()
                if "403" in api_output or "quota" in api_output.lower():
                    logger.error(
                        f"üö´ QUOTA API √âPUIS√â √† l'ann√©e {year}, produit {cmd_code}"
                    )
                    quota_exceeded = True
                    break
                if df is None or len(df) == 0:
                    continue
                if len(df) >= max_records:
                    logger.warning(
                        f"Year {year}, Code {cmd_code:02}: Max records exceeded ({max_records})"
                    )
                    continue
                df_year_cmd.append(df)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur ann√©e {year}, produit {cmd_code}: {e}")
                continue
        if quota_exceeded:
            break
        # Sauvegarder les donn√©es de l'ann√©e
        if df_year_cmd:
            df_year = pd.concat(df_year_cmd, ignore_index=True)
            df_year.to_csv(output_file, index=False)
            logger.info(f"‚úÖ Ann√©e {year}: {len(df_year)} enregistrements sauvegard√©s")
            all_dfs_exports.append(df_year)
        else:
            skipped_years.append(year)
    # R√©sum√© final
    elapsed_min = (time.time() - start_time) / 60
    if quota_exceeded:
        logger.warning("‚ö†Ô∏è DONN√âES INCOMPL√àTES - Quota API √©puis√©")
        if all_dfs_exports:
            df_partial = pd.concat(all_dfs_exports, ignore_index=True)
            logger.info(f"üìä Donn√©es partielles: {len(df_partial)} lignes")
            actual_years = (
                sorted([int(y) for y in df_partial["refYear"].unique()])
                if "refYear" in df_partial.columns
                else []
            )
            logger.info(f"üìä Ann√©es t√©l√©charg√©es: {actual_years}")
            logger.info(f"üö´ Ann√©es skipp√©es: {[int(y) for y in skipped_years]}")
            logger.info(f"‚è±Ô∏è Temps: {elapsed_min:.2f} minutes")
            return df_partial
        return pd.DataFrame()
    if all_dfs_exports:
        df_exports = pd.concat(all_dfs_exports, ignore_index=True)
        actual_years = (
            sorted([int(y) for y in df_exports["refYear"].unique()])
            if "refYear" in df_exports.columns
            else []
        )
        logger.info(f"üìä Ann√©es t√©l√©charg√©es: {actual_years}")
        logger.info(f"üö´ Ann√©es skipp√©es: {[int(y) for y in skipped_years]}")
        logger.info(f"‚è±Ô∏è Temps: {elapsed_min:.2f} minutes")
        return df_exports
    else:
        logger.warning(f"‚ùå Aucune donn√©e r√©cup√©r√©e (‚è±Ô∏è {elapsed_min:.2f} min)")
        return pd.DataFrame()


def clean_iso_codes(df, iso_col="ISO", exclude_iso_codes=None):
    """
    Nettoie les codes ISO dans un DataFrame et exclut √©ventuellement certains codes.

    Args:
        df: DataFrame contenant une colonne de codes ISO
        iso_col: Nom de la colonne contenant les codes ISO
        exclude_iso_codes: Liste ou ensemble de codes ISO √† exclure (optionnel)

    Returns:
        DataFrame avec codes ISO nettoy√©s et exclus
    """
    df[iso_col] = df[iso_col].astype(str).str.strip().str.upper()
    df = df[df[iso_col].notna() & (df[iso_col] != "NAN")]
    df = df[df[iso_col].str.len() == 3]
    if exclude_iso_codes is not None:
        exclude_set = set([code.strip().upper() for code in exclude_iso_codes])
        df = df[~df[iso_col].isin(exclude_set)]
    return df


def validate_dataframe_structure(
    df: pd.DataFrame, required_cols: list, df_name: str = "DataFrame"
) -> bool:
    """
    Valide la structure d'un DataFrame.

    Args:
        df: DataFrame √† valider
        required_cols: Liste des colonnes requises
        df_name: Nom du DataFrame pour les logs

    Returns:
        True si structure valide, False sinon
    """
    if df.empty:
        logger.warning(f"‚ùå {df_name} est vide")
        return False

    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        logger.error(f"‚ùå {df_name} - Colonnes manquantes: {missing_cols}")
        return False

    logger.debug(f"‚úÖ {df_name} - Structure valide ({len(df)} lignes)")
    return True


def get_project_root() -> Path:
    """
    Retourne le r√©pertoire racine du projet.

    Returns:
        Path vers le r√©pertoire racine du projet
    """
    current_file = Path(__file__)

    # Si on est dans pipeline/, remonter d'un niveau
    if current_file.parent.name == "pipeline":
        return current_file.parent.parent
    else:
        return current_file.parent


def ensure_directory_exists(path: Path) -> None:
    """
    S'assure qu'un r√©pertoire existe, le cr√©e sinon.

    Args:
        path: Chemin vers le r√©pertoire
    """
    path.mkdir(parents=True, exist_ok=True)


def log_dataframe_summary(df: pd.DataFrame, name: str) -> None:
    """
    Affiche un r√©sum√© d'un DataFrame dans les logs.

    Args:
        df: DataFrame √† r√©sumer
        name: Nom du DataFrame pour les logs
    """
    if df.empty:
        logger.warning(f"üìä {name}: DataFrame vide")
        return

    logger.info(f"üìä {name}: {len(df):,} lignes √ó {len(df.columns)} colonnes")

    # Afficher les ann√©es si disponibles
    year_cols = [
        col for col in df.columns if col.lower() in ["year", "refyear", "start year"]
    ]
    if year_cols:
        year_col = year_cols[0]
        years = sorted(df[year_col].dropna().unique())
        if years:
            year_range = (
                f"{min(years)}-{max(years)}" if len(years) > 1 else str(years[0])
            )
            logger.info(f"   üìÖ Ann√©es: {year_range}")

    # Afficher les pays si disponibles
    country_cols = [
        col for col in df.columns if col.lower() in ["iso", "reporteriso", "country"]
    ]
    if country_cols:
        country_col = country_cols[0]
        n_countries = df[country_col].nunique()
        logger.info(f"   üåç Pays: {n_countries}")
