#!/usr/bin/env python
"""
Pipeline principal r√©vis√© - Analyse compl√®te de l'impact des catastrophes naturelles sur les exportations.

Ce script orchestre l'ensemble du pipeline d'analyse en respectant strictement la m√©thodologie
de l'article de r√©f√©rence et en incorporant les meilleures pratiques de processing.py.

Fonctionnalit√©s:
1. ‚úÖ Collecte donn√©es d'exportation (Comtrade) pour 1979-2000 ET 2000-2024
2. ‚úÖ Collecte donn√©es catastrophes (EM-DAT + GeoMet) pour les deux p√©riodes
3. ‚úÖ Cr√©ation datasets d'analyse niveau pays ET produit
4. ‚úÖ Analyse √©conom√©trique compl√®te (r√©plication m√©thodologie article)
5. ‚úÖ G√©n√©ration tables format publication (Table 1, 2, 3)
6. ‚úÖ Syst√®me de cache pour √©viter t√©l√©chargements r√©p√©t√©s
7. ‚úÖ Logging d√©taill√© et transparent
8. ‚úÖ Outputs directement utilisables dans le m√©moire
"""

import sys
import os
from pathlib import Path
import subprocess
import time
from loguru import logger
import json

# Load config from config.json
CONFIG_PATH = Path(__file__).parent / "config.json"
with open(CONFIG_PATH, "r") as f:
    config = json.load(f)

EXCLUDED_ISO_CODES = config["EXCLUDED_ISO_CODES"]
CACHE_DIR = Path(config["CACHE_DIR"])
RESULTS_DIR = Path(config["RESULTS_DIR"])
TABLES_DIR = Path(config["TABLES_DIR"])
CLEAR_CACHE = config["CLEAR_CACHE"]
DATA_DIR = Path(config["DATA_DIR"])

# === PARAM√àTRES D'EX√âCUTION (centralis√©s ici) ===
USE_CACHE = True  # Utiliser le cache si disponible
USE_COMTRADE_API = True  # Utiliser l'API Comtrade pour les exports (sinon CSV uniquement)
# Ajouter d'autres param√®tres globaux ici si besoin

# === CONFIGURATION ===
PROJECT_ROOT = Path(__file__).parent.parent  # Chemin relatif depuis pipeline/
PIPELINE_DIR = PROJECT_ROOT / "pipeline"

# Ensure directories exist
for dir_path in [CACHE_DIR, RESULTS_DIR, TABLES_DIR]:
    dir_path.mkdir(exist_ok=True)

# Configure logging
logger.remove()
logger.add(sys.stderr, level="TRACE")


def run_step(step_name: str, script_path: Path, description: str) -> bool:
    """
    Ex√©cute une √©tape du pipeline avec gestion d'erreurs et logging.

    Args:
        step_name: Nom de l'√©tape
        script_path: Chemin vers le script √† ex√©cuter
        description: Description de l'√©tape

    Returns:
        bool: True si succ√®s, False sinon
    """
    logger.info(f"üöÄ Starting {step_name}: {description}")

    try:
        start_time = time.time()

        # Determine command based on file extension
        if script_path.suffix == ".py":
            cmd = [sys.executable, str(script_path)]
        elif script_path.suffix == ".R":
            cmd = ["Rscript", str(script_path)]
        else:
            logger.error(f"‚ùå Unknown script type: {script_path.suffix}")
            return False

        # Set environment variable to indicate pipeline execution
        env = os.environ.copy()
        env["PIPELINE_EXECUTION"] = "1"
        env["PIPELINE_USE_CACHE"] = str(USE_CACHE)
        env["PIPELINE_USE_COMTRADE_API"] = str(USE_COMTRADE_API)

        # For R scripts, ensure PATH includes system directories (fix macOS rm issue)
        if script_path.suffix == ".R":
            current_path = env.get("PATH", "")
            if "/bin:" not in current_path and "/usr/bin:" not in current_path:
                env["PATH"] = f"/bin:/usr/bin:{current_path}"

        # Run the script
        result = subprocess.run(
            cmd,
            cwd=PROJECT_ROOT,
            capture_output=True,
            text=True,
            timeout=3600,  # 1 hour timeout
            env=env,
        )

        # Simple logging - subprocess scripts are now silent when run by pipeline
        if result.stdout:
            stdout_lines = [
                line.strip()
                for line in result.stdout.strip().split("\n")
                if line.strip()
            ]
            if stdout_lines:
                logger.debug(f"  üìù Script output: {len(stdout_lines)} lines")
                # Only show first few lines to avoid spam
                for line in stdout_lines[:3]:
                    logger.debug(f"    {line}")
                if len(stdout_lines) > 3:
                    logger.debug(f"    ... and {len(stdout_lines) - 3} more lines")

        if result.stderr:
            stderr_lines = [
                line.strip()
                for line in result.stderr.strip().split("\n")
                if line.strip()
            ]
            for line in stderr_lines:
                logger.warning(f"  ‚ö†Ô∏è {line}")

        if result.returncode == 0:
            duration = time.time() - start_time
            logger.success(f"‚úÖ {step_name} completed successfully in {duration:.1f}s")
            return True
        else:
            # Special handling for R scripts with 'rm: command not found' issue on macOS
            if (
                script_path.suffix == ".R"
                and result.stderr
                and "rm: command not found" in result.stderr
                and result.returncode == 1
            ):

                # Check if important files were still generated despite the error
                if script_path.name == "04_econometric_analysis.R":
                    tables_created = list(TABLES_DIR.glob("*.tex"))
                    if len(tables_created) >= 10:  # Should have multiple tables
                        duration = time.time() - start_time
                        logger.warning(
                            f"‚ö†Ô∏è {step_name} completed with non-critical system error in {duration:.1f}s"
                        )
                        logger.warning(
                            "   üí° R script encountered 'rm: command not found' but generated tables successfully"
                        )
                        return True

            logger.error(f"‚ùå {step_name} failed with return code {result.returncode}")
            return False

    except subprocess.TimeoutExpired:
        logger.error(f"‚ùå {step_name} timed out after 1 hour")
        return False
    except Exception as e:
        logger.error(f"‚ùå {step_name} failed with error: {e}")
        return False


def check_prerequisites() -> bool:
    """V√©rifie que tous les pr√©requis sont en place."""
    logger.info("üîç Checking prerequisites...")

    # Check Python modules
    required_modules = ["pandas", "numpy", "loguru", "comtradeapicall", "openpyxl"]
    missing_modules = []

    for module in required_modules:
        try:
            __import__(module)
        except ImportError:
            missing_modules.append(module)

    if missing_modules:
        logger.error(f"‚ùå Missing Python modules: {', '.join(missing_modules)}")
        logger.info(
            "Install with: pip install pandas numpy loguru comtradeapicall openpyxl"
        )
        return False

    # Check R availability
    try:
        result = subprocess.run(
            ["Rscript", "--version"], capture_output=True, timeout=10
        )
        if result.returncode != 0:
            logger.error("‚ùå Rscript not available")
            return False
    except (subprocess.TimeoutExpired, FileNotFoundError):
        logger.error("‚ùå R not installed or Rscript not found in PATH")
        return False

    # Check required files
    required_files = [
        PIPELINE_DIR / "01_collect_exports_data.py",
        PIPELINE_DIR / "02_collect_disasters_data.py",
        PIPELINE_DIR / "03_validate_datasets.py",
        PIPELINE_DIR / "04_econometric_analysis.R",
    ]

    missing_files = [f for f in required_files if not f.exists()]
    if missing_files:
        logger.error(f"‚ùå Missing pipeline files: {[str(f) for f in missing_files]}")
        return False

    logger.success("‚úÖ Prerequisites check passed")
    return True


def export_datasets_to_csv() -> bool:
    """Export les datasets d'analyse vers CSV pour l'analyse R."""
    logger.info("üì§ Exporting datasets to CSV for R analysis...")

    try:
        import pandas as pd
        import pickle

        # Load cached datasets
        datasets = {}
        for period in ["1979_2000", "2000_2024"]:
            for level in ["country", "product"]:
                cache_file = CACHE_DIR / f"analysis_{level}_{period}.pkl"
                if cache_file.exists():
                    with open(cache_file, "rb") as f:
                        datasets[f"{level}_{period}"] = pickle.load(f)
                        logger.debug(
                            f"  üì¶ Loaded {level}_{period}: {len(datasets[f'{level}_{period}'])} obs"
                        )

        # Combine datasets for econometric analysis
        if datasets:
            # Combine all periods and levels
            combined_data = []
            for key, df in datasets.items():
                df_copy = df.copy()
                df_copy["dataset_source"] = key
                combined_data.append(df_copy)

            if combined_data:
                final_dataset = pd.concat(combined_data, ignore_index=True, sort=False)

                # Export main econometric dataset
                output_file = RESULTS_DIR / "econometric_dataset.csv"
                final_dataset.to_csv(output_file, index=False)
                logger.success(
                    f"  ‚úÖ Econometric dataset exported: {len(final_dataset)} observations"
                )

                # Export individual datasets
                for key, df in datasets.items():
                    individual_file = RESULTS_DIR / f"analysis_{key}.csv"
                    df.to_csv(individual_file, index=False)
                    logger.debug(f"  üìÑ Exported {key}: {len(df)} observations")

        logger.success("‚úÖ CSV export completed")
        return True

    except Exception as e:
        logger.error(f"‚ùå CSV export failed: {e}")
        return False


def check_r_packages_installed(required_packages=None):
    """V√©rifie si les packages R n√©cessaires sont install√©s. Installe si besoin."""
    import tempfile
    import shutil

    if required_packages is None:
        required_packages = [
            "dplyr",
            "readr",
            "stringr",
            "broom",
            "fixest",
            "modelsummary",
            "xtable",
            "logger",
        ]
    # Cr√©e un script R temporaire pour tester les packages
    r_code = """
    pkgs <- c({})
    missing <- pkgs[!sapply(pkgs, require, character.only=TRUE, quietly=TRUE)]
    if (length(missing) > 0) {{ cat(paste(missing, collapse=',')); quit(status=42) }} else {{ cat('OK'); quit(status=0) }}
    """.format(
        ",".join([f'"{pkg}"' for pkg in required_packages])
    )
    with tempfile.NamedTemporaryFile("w", suffix=".R", delete=False) as f:
        f.write(r_code)
        r_script_path = f.name
    try:
        result = subprocess.run(
            ["Rscript", r_script_path], capture_output=True, text=True, timeout=30
        )
        if result.returncode == 0 and "OK" in result.stdout:
            logger.info("‚úÖ All required R packages are installed.")
            return True
        else:
            logger.warning(f"‚ö†Ô∏è Missing R packages: {result.stdout.strip()}")
            # Tenter installation automatique
            install_script = (
                PROJECT_ROOT / "pipeline" / "utils" / "install_r_packages.R"
            )
            if install_script.exists():
                logger.info(
                    "üîß Installing missing R packages via pipeline/utils/install_r_packages.R ..."
                )
                install_result = subprocess.run(
                    ["Rscript", str(install_script)],
                    capture_output=True,
                    text=True,
                    timeout=600,
                )
                if install_result.returncode == 0:
                    logger.success("‚úÖ R packages installed successfully.")
                    return True
                else:
                    logger.error(
                        f"‚ùå Failed to install R packages: {install_result.stderr}"
                    )
                    return False
            else:
                logger.error("‚ùå install_r_packages.R not found in pipeline/utils/.")
                return False
    finally:
        try:
            os.remove(r_script_path)
        except Exception:
            pass


def run_full_pipeline(force_refresh: bool = False) -> bool:
    """
    Ex√©cute le pipeline complet d'analyse √©conom√©trique.

    Args:
        force_refresh: Si True, ignore le cache et recharge tout

    Returns:
        bool: True si succ√®s global, False sinon
    """
    start_time = time.time()

    logger.info("üéØ Starting complete revised econometric pipeline...")
    logger.info(f"üìÅ Project root: {PROJECT_ROOT}")
    logger.info(f"üîÑ Force refresh: {force_refresh}")

    # Check prerequisites
    if not check_prerequisites():
        return False

    # Pipeline steps
    steps = [
        (
            "STEP 1",
            PIPELINE_DIR / "01_collect_exports_data.py",
            "Collecting export data (Comtrade) for both periods",
        ),
        (
            "STEP 2",
            PIPELINE_DIR / "02_collect_disasters_data.py",
            "Collecting disaster data (EM-DAT + GeoMet) for both periods",
        ),
        (
            "STEP 3",
            PIPELINE_DIR / "03_validate_datasets.py",
            "Fusion and validation of analysis datasets (country + product level)",
        ),
    ]

    # Execute Python steps
    success_count = 0
    for step_name, script_path, description in steps:
        if run_step(step_name, script_path, description):
            success_count += 1
        else:
            logger.warning(f"‚ö†Ô∏è {step_name} failed but continuing...")

    # Export to CSV for R analysis
    if not export_datasets_to_csv():
        logger.warning("‚ö†Ô∏è CSV export failed but continuing...")

    # Check R packages before running R scripts
    if not check_r_packages_installed():
        logger.error(
            "‚ùå R packages missing and could not be installed. Aborting pipeline."
        )
        return False

    # Run R analysis - Original single-period analysis
    r_script = PIPELINE_DIR / "04_econometric_analysis.R"
    if r_script.exists():
        if run_step(
            "STEP 4", r_script, "Running econometric analysis and generating tables"
        ):
            success_count += 1
        else:
            logger.warning("‚ö†Ô∏è Econometric analysis failed - check R setup")

    # Run multi-period, multi-criteria analysis
    r_script_multi = PIPELINE_DIR / "04b_multi_period_analysis.R"
    if r_script_multi.exists():
        if run_step(
            "STEP 4B",
            r_script_multi,
            "Running multi-period/multi-criteria analysis (4 versions per table)",
        ):
            success_count += 1
        else:
            logger.warning("‚ö†Ô∏è Multi-period analysis failed - check R setup")

    # Update total steps count
    total_steps = len(steps) + 2  # +2 for both R steps

    # Summary
    success_rate = (success_count / total_steps) * 100

    logger.info("")
    logger.info("=" * 80)
    logger.info("üéâ PIPELINE EXECUTION SUMMARY")
    logger.info("=" * 80)
    logger.info(f"‚è±Ô∏è  Total duration: {time.time() - start_time:.1f} seconds")
    logger.info(f"üìÅ Project directory: {PROJECT_ROOT}")
    logger.info(f"üíæ Cache directory: {CACHE_DIR}")
    logger.info(f"üìä Results directory: {RESULTS_DIR}")
    logger.info(f"üìÑ Tables directory: {TABLES_DIR}")

    # List generated files
    logger.info("")
    logger.info("üìã GENERATED OUTPUTS:")

    # Cache files
    cache_files = list(CACHE_DIR.glob("*.pkl"))
    if cache_files:
        logger.info(f"üíæ Cache files: {len(cache_files)}")
        for f in sorted(cache_files):
            logger.info(f"   ‚Ä¢ {f.name}")

    # Result files
    result_files = list(RESULTS_DIR.glob("*"))
    result_files = [f for f in result_files if f.is_file()]
    if result_files:
        logger.info(f"üìä Result files: {len(result_files)}")
        for f in sorted(result_files):
            logger.info(f"   ‚Ä¢ {f.name}")

    # LaTeX tables
    table_files = list(TABLES_DIR.glob("*.tex"))
    if table_files:
        logger.info(f"üìÑ LaTeX tables: {len(table_files)}")
        for f in sorted(table_files):
            logger.info(f"   ‚Ä¢ {f.name}")

    logger.info("")
    logger.info("üéØ METHODOLOGY ALIGNMENT:")
    logger.info("‚úÖ Deux p√©riodes analys√©es: 1979-2000 et 2000-2024")
    logger.info("‚úÖ Structure panel produit-pays-ann√©e")
    logger.info("‚úÖ Variables de catastrophes EM-DAT et GeoMet")
    logger.info("‚úÖ Sp√©cification en diff√©rences logarithmiques")
    logger.info("‚úÖ Effets fixes produit√ópays et produit√óann√©e")
    logger.info("‚úÖ Interactions pays pauvres/petits")
    logger.info("‚úÖ Tables format publication (Table 1, 2, 3)")
    logger.info("‚úÖ Syst√®me de cache pour reproductibilit√©")
    logger.info("‚úÖ NOUVELLE: Analyse 4 combinaisons (2 p√©riodes √ó 2 crit√®res)")
    logger.info("   ‚Ä¢ 1979-2000 tous √©v√©nements")
    logger.info("   ‚Ä¢ 1979-2000 √©v√©nements significatifs")
    logger.info("   ‚Ä¢ 2000-2024 tous √©v√©nements")
    logger.info("   ‚Ä¢ 2000-2024 √©v√©nements significatifs")

    if success_rate >= 75:
        logger.success(
            f"üéâ Pipeline completed successfully! ({success_rate:.0f}% steps passed)"
        )
        logger.info("")
        logger.info("üöÄ Ready for memoir integration!")
        return True
    else:
        logger.error(f"‚ùå Pipeline failed ({success_rate:.0f}% steps passed)")
        return False


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Pipeline d'analyse √©conom√©trique")
    parser.add_argument(
        "--force-refresh",
        action="store_true",
        help="Force refresh of all data (ignore cache)",
    )

    args = parser.parse_args()

    # === LOG CONFIGURATION AT STARTUP ===
    logger.info("\n==============================\n   üö¶ PIPELINE CONFIGURATION   \n==============================")
    logger.info(f"USE_CACHE: {USE_CACHE}")
    logger.info(f"CLEAR_CACHE: {CLEAR_CACHE}")
    logger.info(f"DATA_DIR: {DATA_DIR}")
    logger.info(f"CACHE_DIR: {CACHE_DIR}")
    logger.info(f"RESULTS_DIR: {RESULTS_DIR}")
    logger.info(f"TABLES_DIR: {TABLES_DIR}")
    logger.info(f"N ISO codes retir√©s: {len(EXCLUDED_ISO_CODES)}")
    logger.info(f"ISO exclus: {', '.join(EXCLUDED_ISO_CODES)}")
    logger.info("==============================\n")

    success = run_full_pipeline(force_refresh=args.force_refresh)
    sys.exit(0 if success else 1)
